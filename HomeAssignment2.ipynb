{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pdelfidali/SNDL/blob/main/HomeAssignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Home assignment 2\n",
        "## Piotr del Fidali\n",
        "\n",
        "### Load MNIST dataset\n",
        "We load MNIST dataset using function from a keras library. By default, it divides data to 60.000 record for train and 10.000 test sets. We also change the types to float and divide them by 255, so the range of values is (0, 1) rather than (0, 255). Next, we reshape data so the single observation is vector with length of 784 (not matrix with 28x28 size)."
      ],
      "metadata": {
        "collapsed": false,
        "id": "97a08d077cdf6590"
      },
      "id": "97a08d077cdf6590"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Training data shape: (60000, 784)\n",
            "Testing data shape: (10000, 784)\n"
          ]
        }
      ],
      "source": [
        "from keras.src.utils import to_categorical\n",
        "from keras.datasets.mnist import load_data\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = load_data()\n",
        "x_train = x_train.astype('float32') /255.\n",
        "x_test = x_test.astype('float32') /255.\n",
        "\n",
        "x_train = x_train.reshape(60000, 28*28)\n",
        "x_test = x_test.reshape(10000, 28*28)\n",
        "\n",
        "print(\"Training data shape:\", x_train.shape)\n",
        "print(\"Testing data shape:\", x_test.shape)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-11T20:01:34.610706500Z",
          "start_time": "2023-12-11T20:01:23.616140Z"
        },
        "id": "4b1aaebb48a27bd8",
        "outputId": "256c34ef-237c-436e-f048-92163753520c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4b1aaebb48a27bd8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the function that will select only observations for given digits. It will also ensure that the length of y vector is the same as the number of digits in given range. To work properly, argument digit_range should be next digits (ex. [1, 2, 3] but not [2, 5, 7])."
      ],
      "metadata": {
        "collapsed": false,
        "id": "2786cf6c92961b3e"
      },
      "id": "2786cf6c92961b3e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Amount of samples in training datasets after slicing:\n",
            "0-2: (18623, 3) - 31.04% of full training\n",
            "3-5: (17394, 3) - 28.99% of full training\n",
            "6-9: (23983, 4) - 39.97% of full training\n",
            "Amount of samples in test datasets after slicing:\n",
            "0-2: (3147, 3) - 31.47% of full test\n",
            "3-5: (2884, 3) - 28.84% of full test\n",
            "6-9: (3969, 4) - 39.69% of full test\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def vertical_slice_data(x_data, y_data, digit_range):\n",
        "    indices = np.where(np.isin(y_data, digit_range))[0]\n",
        "    y_sliced = y_data[indices]\n",
        "    y_sliced = y_sliced - min(digit_range)\n",
        "    return x_data[indices], to_categorical(y_sliced)\n",
        "\n",
        "x_train_0_to_2, y_train_0_to_2 = vertical_slice_data(x_train, y_train, [0, 1, 2])\n",
        "x_train_3_to_5, y_train_3_to_5 = vertical_slice_data(x_train, y_train, [3, 4, 5])\n",
        "x_train_6_to_9, y_train_6_to_9 = vertical_slice_data(x_train, y_train, [6, 7, 8, 9])\n",
        "\n",
        "x_test_0_to_2, y_test_0_to_2 = vertical_slice_data(x_test, y_test, [0, 1, 2])\n",
        "x_test_3_to_5, y_test_3_to_5 = vertical_slice_data(x_test, y_test, [3, 4, 5])\n",
        "x_test_6_to_9, y_test_6_to_9 = vertical_slice_data(x_test, y_test, [6, 7, 8, 9])\n",
        "\n",
        "\n",
        "print(\"Amount of samples in training datasets after slicing:\")\n",
        "print(f'0-2: {y_train_0_to_2.shape} - {len(y_train_0_to_2)/len(y_train):.2%} of full training')\n",
        "print(f'3-5: {y_train_3_to_5.shape} - {len(y_train_3_to_5)/len(y_train):.2%} of full training')\n",
        "print(f'6-9: {y_train_6_to_9.shape} - {len(y_train_6_to_9)/len(y_train):.2%} of full training')\n",
        "\n",
        "print(\"Amount of samples in test datasets after slicing:\")\n",
        "print(f'0-2: {y_test_0_to_2.shape} - {len(y_test_0_to_2)/len(y_test):.2%} of full test')\n",
        "print(f'3-5: {y_test_3_to_5.shape} - {len(y_test_3_to_5)/len(y_test):.2%} of full test')\n",
        "print(f'6-9: {y_test_6_to_9.shape} - {len(y_test_6_to_9)/len(y_test):.2%} of full test')"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-11T20:01:34.793246300Z",
          "start_time": "2023-12-11T20:01:34.593544600Z"
        },
        "id": "9e924fd0bcea079a",
        "outputId": "9a9d0acd-ddbd-4319-8794-1f3ce163ce74"
      },
      "id": "9e924fd0bcea079a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train local models using each remote local dataset\n",
        "\n",
        "We train three models for 0–2, 3–5, 6–9 digits. We  "
      ],
      "metadata": {
        "collapsed": false,
        "id": "a85c8db8389bac15"
      },
      "id": "a85c8db8389bac15"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From D:\\Studia\\Semestr 2\\SNDL\\venv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "WARNING:tensorflow:From D:\\Studia\\Semestr 2\\SNDL\\venv\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 512)               401920    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 1539      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 403459 (1.54 MB)\n",
            "Trainable params: 403459 (1.54 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From D:\\Studia\\Semestr 2\\SNDL\\venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "WARNING:tensorflow:From D:\\Studia\\Semestr 2\\SNDL\\venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "146/146 [==============================] - 2s 7ms/step - loss: 0.0645 - accuracy: 0.9797\n",
            "Epoch 2/10\n",
            "146/146 [==============================] - 1s 5ms/step - loss: 0.0214 - accuracy: 0.9935\n",
            "Epoch 3/10\n",
            "146/146 [==============================] - 1s 6ms/step - loss: 0.0129 - accuracy: 0.9960\n",
            "Epoch 4/10\n",
            "146/146 [==============================] - 1s 6ms/step - loss: 0.0089 - accuracy: 0.9974\n",
            "Epoch 5/10\n",
            "146/146 [==============================] - 1s 6ms/step - loss: 0.0063 - accuracy: 0.9986\n",
            "Epoch 6/10\n",
            "146/146 [==============================] - 1s 7ms/step - loss: 0.0042 - accuracy: 0.9990\n",
            "Epoch 7/10\n",
            "146/146 [==============================] - 1s 6ms/step - loss: 0.0028 - accuracy: 0.9994\n",
            "Epoch 8/10\n",
            "146/146 [==============================] - 1s 6ms/step - loss: 0.0024 - accuracy: 0.9994\n",
            "Epoch 9/10\n",
            "146/146 [==============================] - 1s 6ms/step - loss: 9.9957e-04 - accuracy: 0.9999\n",
            "Epoch 10/10\n",
            "117/146 [=======================>......] - ETA: 0s - loss: 4.7590e-04 - accuracy: 1.0000"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dense\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, recall_score, precision_score, accuracy_score\n",
        "\n",
        "model_0_to_2 = Sequential()\n",
        "model_0_to_2.add(Dense(512, activation='relu', input_shape=(28*28, )))\n",
        "model_0_to_2.add(Dense(3, activation='softmax'))\n",
        "model_0_to_2.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model_0_to_2.summary()\n",
        "\n",
        "model_0_to_2.fit(x_train_0_to_2, y_train_0_to_2, epochs=10, batch_size=128)\n",
        "test_loss, test_accu = model_0_to_2.evaluate(x_test_0_to_2, y_test_0_to_2)\n",
        "y_pred_0_to_2 = model_0_to_2.predict(x_test_0_to_2)\n",
        "test_recall = recall_score(y_test_0_to_2.argmax(1), y_pred_0_to_2.argmax(1), average=None)\n",
        "test_precision = precision_score(y_test_0_to_2.argmax(1), y_pred_0_to_2.argmax(1), average=None)"
      ],
      "metadata": {
        "is_executing": true,
        "ExecuteTime": {
          "start_time": "2023-12-11T20:01:34.790247400Z"
        },
        "id": "8d4e9d198af68fa3",
        "outputId": "b6ae542d-aaf7-4533-c9d1-a4b7945e9450"
      },
      "id": "8d4e9d198af68fa3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "print('Model for 0, 1 and 2 digits classification')\n",
        "print(f'\\tTest loss: {test_loss:5f}\\n\\tTest accuracy: {test_accu:5f}')\n",
        "for class_idx, recall in enumerate(test_recall):\n",
        "    print(f\"\\tRecall for Class {class_idx}: {recall:.5f}\")\n",
        "for class_idx, recall in enumerate(test_precision):\n",
        "    print(f\"\\tPrecision for Class {class_idx}: {recall:.5f}\")"
      ],
      "metadata": {
        "is_executing": true,
        "id": "5c6882b9202823ab"
      },
      "id": "5c6882b9202823ab"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "cm_0_to_2 = confusion_matrix(y_test_0_to_2.argmax(1), y_pred_0_to_2.argmax(1))\n",
        "disp = ConfusionMatrixDisplay(cm_0_to_2, display_labels=[0, 1, 2])\n",
        "disp.plot(cmap='winter')"
      ],
      "metadata": {
        "is_executing": true,
        "id": "971354bbd73fad51"
      },
      "id": "971354bbd73fad51"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "model_3_to_5 = Sequential()\n",
        "model_3_to_5.add(Dense(512, activation='relu', input_shape=(28*28, )))\n",
        "model_3_to_5.add(Dense(3, activation='softmax'))\n",
        "model_3_to_5.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model_3_to_5.summary()\n",
        "\n",
        "model_3_to_5.fit(x_train_3_to_5, y_train_3_to_5, epochs=10, batch_size=128)\n",
        "test_loss, test_accu = model_3_to_5.evaluate(x_test_3_to_5, y_test_3_to_5)\n",
        "y_pred_3_to_5 = model_3_to_5.predict(x_test_3_to_5)\n",
        "test_recall = recall_score(y_test_3_to_5.argmax(1), y_pred_3_to_5.argmax(1), average=None)\n",
        "test_precision = precision_score(y_test_3_to_5.argmax(1), y_pred_3_to_5.argmax(1), average=None)"
      ],
      "metadata": {
        "is_executing": true,
        "id": "d6c76c615eb7fe99"
      },
      "id": "d6c76c615eb7fe99"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "print('Model for 3, 4 and 5 digits classification')\n",
        "print(f'\\tTest loss: {test_loss:5f} \\n\\tAccuracy: {test_accu:.5f}')\n",
        "for class_idx, recall in enumerate(test_recall):\n",
        "    print(f\"\\tRecall for Class {class_idx + 3}: {recall:.5f}\")\n",
        "for class_idx, recall in enumerate(test_precision):\n",
        "    print(f\"\\tPrecision for Class {class_idx + 3}: {recall:.5f}\")"
      ],
      "metadata": {
        "is_executing": true,
        "id": "a05732855e2ae56e"
      },
      "id": "a05732855e2ae56e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "cm_3_to_5 = confusion_matrix(y_test_3_to_5.argmax(1), y_pred_3_to_5.argmax(1))\n",
        "disp = ConfusionMatrixDisplay(cm_3_to_5, display_labels=[3, 4, 5])\n",
        "disp.plot(cmap='winter')"
      ],
      "metadata": {
        "is_executing": true,
        "id": "7210111dee8b2e9b"
      },
      "id": "7210111dee8b2e9b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "model_6_to_9 = Sequential()\n",
        "model_6_to_9.add(Dense(512, activation='relu', input_shape=(28*28, )))\n",
        "model_6_to_9.add(Dense(4, activation='softmax'))\n",
        "model_6_to_9.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model_6_to_9.summary()\n",
        "\n",
        "\n",
        "model_6_to_9.fit(x_train_6_to_9, y_train_6_to_9, epochs=10, batch_size=128)\n",
        "test_loss, test_accu = model_6_to_9.evaluate(x_test_6_to_9, y_test_6_to_9)\n",
        "y_pred_6_to_9 = model_6_to_9.predict(x_test_6_to_9)\n",
        "test_recall = recall_score(y_test_6_to_9.argmax(1), y_pred_6_to_9.argmax(1), average=None)\n",
        "test_precision = precision_score(y_test_6_to_9.argmax(1), y_pred_6_to_9.argmax(1), average=None)"
      ],
      "metadata": {
        "is_executing": true,
        "id": "6b25f04a83d7bcde"
      },
      "id": "6b25f04a83d7bcde"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "print('Model for 6, 7, 8 and 9 digits classification')\n",
        "print(f'\\tTest loss: {test_loss:5f} \\n\\tAccuracy: {test_accu:.5f}')\n",
        "for class_idx, recall in enumerate(test_recall):\n",
        "    print(f\"\\tRecall for Class {class_idx + 6}: {recall:.5f}\")\n",
        "for class_idx, recall in enumerate(test_precision):\n",
        "    print(f\"\\tPrecision for Class {class_idx + 6}: {recall:.5f}\")"
      ],
      "metadata": {
        "is_executing": true,
        "id": "64f829a6172b2d25"
      },
      "id": "64f829a6172b2d25"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "cm_6_to_9 = confusion_matrix(y_test_6_to_9.argmax(1), y_pred_6_to_9.argmax(1))\n",
        "disp = ConfusionMatrixDisplay(cm_6_to_9, display_labels=[6, 7, 8, 9])\n",
        "disp.plot(cmap='winter')"
      ],
      "metadata": {
        "is_executing": true,
        "id": "e1a61f11b1ce5933"
      },
      "id": "e1a61f11b1ce5933"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aggregate the local models into a global model\n",
        "\n",
        "We create an Input layer that will take a single image and will pass it to each of our defined models. Then we combine outputs of models to a single vector of length 10, because of making models take next digits, we do not have to worry about any mapping."
      ],
      "metadata": {
        "collapsed": false,
        "id": "8453ef2e849e7905"
      },
      "id": "8453ef2e849e7905"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, concatenate\n",
        "\n",
        "common_input = Input(shape=(784,))\n",
        "\n",
        "output_model_1 = model_0_to_2(common_input)\n",
        "output_model_2 = model_3_to_5(common_input)\n",
        "output_model_3 = model_6_to_9(common_input)\n",
        "\n",
        "combined_output = concatenate([output_model_1, output_model_2, output_model_3])\n",
        "\n",
        "combined_model = Model(inputs=common_input, outputs=combined_output)"
      ],
      "metadata": {
        "is_executing": true,
        "id": "1040234aa3a3b5c"
      },
      "id": "1040234aa3a3b5c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "y_pred = combined_model.predict(x_test)\n",
        "test_accu = accuracy_score(y_test, y_pred.argmax(1))\n",
        "test_recall = recall_score(y_test, y_pred.argmax(1), average=None)\n",
        "test_precision = precision_score(y_test, y_pred.argmax(1), average=None)"
      ],
      "metadata": {
        "is_executing": true,
        "id": "bbd7565f14d80eb4"
      },
      "id": "bbd7565f14d80eb4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "print('Combined model digits classification')\n",
        "print(f'\\tAccuracy: {test_accu:.5f}')\n",
        "for class_idx, recall in enumerate(test_recall):\n",
        "    print(f\"\\tRecall for Class {class_idx}: {recall:.5f}\")\n",
        "for class_idx, recall in enumerate(test_precision):\n",
        "    print(f\"\\tPrecision for Class {class_idx}: {recall:.5f}\")"
      ],
      "metadata": {
        "is_executing": true,
        "id": "2cc1f296c51a959f"
      },
      "id": "2cc1f296c51a959f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "cm_combined_model = confusion_matrix(to_categorical(y_test).argmax(1), y_pred.argmax(1))\n",
        "disp = ConfusionMatrixDisplay(cm_combined_model)\n",
        "disp.plot(cmap='winter')"
      ],
      "metadata": {
        "is_executing": true,
        "id": "3eadf567386882f4"
      },
      "id": "3eadf567386882f4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The combined model is not performing as well as models that predict only three or four classes. Although its performance is not as good, we can see that it does not make miss classifications inside the group it was learned in. For example, for digit range 0–2 there are only two miss classifications as another digit from the same range. For the second range it is also two and for third it's 3.  There is a lot '2' misclassified as '3' we can see that in confusion matrix. Recall value is smallest for class '2,' precision is worst for both class '2' and '3.' Overall, the model is performing well."
      ],
      "metadata": {
        "collapsed": false,
        "id": "9ff9c04967acd9ad"
      },
      "id": "9ff9c04967acd9ad"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model trained on the full dataset"
      ],
      "metadata": {
        "collapsed": false,
        "id": "946ecbc8fcf0b790"
      },
      "id": "946ecbc8fcf0b790"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "y_train = to_categorical(y_train)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(512, activation='relu', input_shape=(28*28, )))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=128)\n",
        "y_pred = model.predict(x_test)\n",
        "test_accu = accuracy_score(y_test, y_pred.argmax(1))\n",
        "test_recall = recall_score(y_test, y_pred.argmax(1), average=None)\n",
        "test_precision = precision_score(y_test, y_pred.argmax(1), average=None)"
      ],
      "metadata": {
        "is_executing": true,
        "id": "8cf7fd5076403728"
      },
      "id": "8cf7fd5076403728"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "print('Model trained for full dataset digits classification')\n",
        "print(f'\\tAccuracy: {test_accu:.5f}')\n",
        "for class_idx, recall in enumerate(test_recall):\n",
        "    print(f\"\\tRecall for Class {class_idx}: {recall:.5f}\")\n",
        "for class_idx, recall in enumerate(test_precision):\n",
        "    print(f\"\\tPrecision for Class {class_idx}: {recall:.5f}\")"
      ],
      "metadata": {
        "is_executing": true,
        "id": "8cadd20102fe149c"
      },
      "id": "8cadd20102fe149c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "cm_full_model = confusion_matrix(y_test, y_pred.argmax(1))\n",
        "disp = ConfusionMatrixDisplay(cm_full_model)\n",
        "disp.plot(cmap='winter')"
      ],
      "metadata": {
        "is_executing": true,
        "id": "91688469fd1fa96"
      },
      "id": "91688469fd1fa96"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model trained on full dataset, performance is much better than the combined model.\n",
        "\n",
        "## Conclusion\n",
        "Training the models for subsets of data might be beneficial if the dataset is huge:\n",
        "   - it can save time for training\n",
        "   - it can help to remove some classification errors, as we could observe there was little number of errors inside a specific group.\n",
        "\n",
        "\n",
        "The model performance (all metrics) is better if we train the model on full dataset."
      ],
      "metadata": {
        "collapsed": false,
        "id": "a1c7c2d584d41a6b"
      },
      "id": "a1c7c2d584d41a6b"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}