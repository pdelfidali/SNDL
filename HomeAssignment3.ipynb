{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pdelfidali/SNDL/blob/main/HomeAssignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Home assignment 3 - Transfer Learning - Cats vs Dogs\n",
        "## Piotr del Fidali | Kamil Zaremba\n",
        "\n",
        "We decided to use transfer learning to classify cats and dogs. We will use the dataset from [Kaggle](https://www.kaggle.com/datasets/chetankv/dogs-cats-images), it contains 10000 cat images and 10000 dog images, divided into 80% training and 20 % test data sets. We will use Google Colab with GPU for faster training process."
      ],
      "metadata": {
        "collapsed": false,
        "id": "ee2e087e083c09bb"
      },
      "id": "ee2e087e083c09bb"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfVnTRVOWBnQ",
        "outputId": "d4cdeaee-5f77-486c-b6e4-28313806cd70"
      },
      "id": "LfVnTRVOWBnQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "K33bfM7oXD7j",
        "outputId": "4d575804-2ecf-414c-b042-a0a82ffbc071"
      },
      "id": "K33bfM7oXD7j",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-02d46d67-712e-4ad6-a023-681c8dda4c5a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-02d46d67-712e-4ad6-a023-681c8dda4c5a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"piotrdelfidali\",\"key\":\"00f2a9bfc77e9e7b8b3a2341bc997be7\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "RCDCsV4nXJUr"
      },
      "id": "RCDCsV4nXJUr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d chetankv/dogs-cats-images\n",
        "!unzip dogs-cats-images.zip"
      ],
      "metadata": {
        "id": "1svAMMPNXMM6"
      },
      "id": "1svAMMPNXMM6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keras\n",
        "In Keras there is ImageDataGenerator class that will be used. It rescales, resizes and labels the input images from given directory. We create one generator for each train and test data. We will use two base models:\n",
        "- VGG16 which is a deep convolutional neural network architecture known for its simplicity, featuring 16 layers with convolutional layers stacked on top of each other in increasing depth.\n",
        "- InceptionV3 which is a more complex architecture that utilizes inception modules, combining filters of different sizes in the same network layer to capture information at various scales.\n",
        "On top of each base model, we add four layers. The First one is Flatten, we convert the multidimensional input layer and convert into a one-dimensional array. The Next three are Dense layers, the final one output size is 1 as we are doing binary classification."
      ],
      "metadata": {
        "collapsed": false,
        "id": "4ced585a3c54b48b"
      },
      "id": "4ced585a3c54b48b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63dda25df301460",
      "metadata": {
        "collapsed": true,
        "ExecuteTime": {
          "end_time": "2024-01-15T18:03:03.422056500Z",
          "start_time": "2024-01-15T18:02:46.332008800Z"
        },
        "id": "63dda25df301460"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1. / 255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8000 images belonging to 2 classes.\n",
            "Found 2000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "train_generator = train_datagen.flow_from_directory('/content/dataset/training_set',\n",
        "                                                    batch_size = 32,\n",
        "                                                    class_mode = 'binary',\n",
        "                                                    target_size = (224, 224))\n",
        "\n",
        "\n",
        "validation_generator =  validation_datagen.flow_from_directory( '/content/dataset/test_set',\n",
        "                                                          batch_size  = 32,\n",
        "                                                          class_mode  = 'binary',\n",
        "                                                          target_size = (224, 224))"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-15T18:03:04.088157600Z",
          "start_time": "2024-01-15T18:03:03.421050100Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0448641a4d20a74",
        "outputId": "6cea7770-79a9-47d5-ad0f-cc584e5268e5"
      },
      "id": "b0448641a4d20a74"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ILusV_tgaYvV"
      },
      "id": "ILusV_tgaYvV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               3211392   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 17934401 (68.41 MB)\n",
            "Trainable params: 3219713 (12.28 MB)\n",
            "Non-trainable params: 14714688 (56.13 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dense, Flatten\n",
        "from keras.applications.vgg16 import VGG16\n",
        "\n",
        "keras_vgg16 = Sequential()\n",
        "keras_vgg16.add(VGG16(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3)))\n",
        "keras_vgg16.add(Flatten())\n",
        "keras_vgg16.add(Dense(128, activation=\"relu\"))\n",
        "keras_vgg16.add(Dense(64, activation=\"relu\"))\n",
        "keras_vgg16.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "keras_vgg16.layers[0].trainable = False\n",
        "\n",
        "keras_vgg16.summary()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-15T18:04:50.546238800Z",
          "start_time": "2024-01-15T18:04:47.484528200Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2177ca8f107be11b",
        "outputId": "6cdaa79b-8914-4af0-f8b4-eaafc2f7b9f8"
      },
      "id": "2177ca8f107be11b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "250/250 [==============================] - 43s 168ms/step - loss: 0.0398 - accuracy: 0.9890 - val_loss: 0.6237 - val_accuracy: 0.8835\n",
            "Epoch 2/50\n",
            "250/250 [==============================] - 44s 174ms/step - loss: 0.0343 - accuracy: 0.9926 - val_loss: 0.9371 - val_accuracy: 0.8820\n",
            "Epoch 3/50\n",
            "250/250 [==============================] - 44s 176ms/step - loss: 0.0319 - accuracy: 0.9924 - val_loss: 0.3824 - val_accuracy: 0.9360\n",
            "Epoch 4/50\n",
            "250/250 [==============================] - 43s 170ms/step - loss: 0.0267 - accuracy: 0.9939 - val_loss: 0.4041 - val_accuracy: 0.9310\n",
            "Epoch 5/50\n",
            "250/250 [==============================] - 44s 177ms/step - loss: 0.0263 - accuracy: 0.9950 - val_loss: 0.4219 - val_accuracy: 0.9325\n",
            "Epoch 6/50\n",
            "250/250 [==============================] - 43s 171ms/step - loss: 0.0382 - accuracy: 0.9929 - val_loss: 0.4144 - val_accuracy: 0.9310\n",
            "Epoch 7/50\n",
            "250/250 [==============================] - 42s 168ms/step - loss: 0.0221 - accuracy: 0.9964 - val_loss: 0.5938 - val_accuracy: 0.9250\n",
            "Epoch 8/50\n",
            "250/250 [==============================] - 42s 168ms/step - loss: 0.0260 - accuracy: 0.9956 - val_loss: 0.4181 - val_accuracy: 0.9375\n",
            "Epoch 9/50\n",
            "250/250 [==============================] - 42s 169ms/step - loss: 0.0159 - accuracy: 0.9965 - val_loss: 0.5207 - val_accuracy: 0.9335\n",
            "Epoch 10/50\n",
            "250/250 [==============================] - 42s 167ms/step - loss: 0.0214 - accuracy: 0.9958 - val_loss: 0.5148 - val_accuracy: 0.9345\n",
            "Epoch 11/50\n",
            "250/250 [==============================] - 42s 167ms/step - loss: 0.0226 - accuracy: 0.9966 - val_loss: 0.5326 - val_accuracy: 0.9345\n",
            "Epoch 12/50\n",
            "250/250 [==============================] - 42s 168ms/step - loss: 0.0078 - accuracy: 0.9984 - val_loss: 0.6807 - val_accuracy: 0.9350\n",
            "Epoch 13/50\n",
            "250/250 [==============================] - 42s 169ms/step - loss: 0.0241 - accuracy: 0.9958 - val_loss: 0.5440 - val_accuracy: 0.9270\n",
            "Epoch 14/50\n",
            "250/250 [==============================] - 42s 167ms/step - loss: 0.0182 - accuracy: 0.9971 - val_loss: 0.6117 - val_accuracy: 0.9315\n",
            "Epoch 15/50\n",
            "250/250 [==============================] - 44s 174ms/step - loss: 0.0069 - accuracy: 0.9987 - val_loss: 0.6118 - val_accuracy: 0.9320\n",
            "Epoch 16/50\n",
            "250/250 [==============================] - 42s 167ms/step - loss: 0.0171 - accuracy: 0.9967 - val_loss: 0.7009 - val_accuracy: 0.9285\n",
            "Epoch 17/50\n",
            "250/250 [==============================] - 42s 167ms/step - loss: 0.0214 - accuracy: 0.9967 - val_loss: 0.6238 - val_accuracy: 0.9295\n",
            "Epoch 18/50\n",
            "250/250 [==============================] - 42s 167ms/step - loss: 2.8610e-05 - accuracy: 1.0000 - val_loss: 0.6927 - val_accuracy: 0.9345\n",
            "Epoch 19/50\n",
            "250/250 [==============================] - 44s 175ms/step - loss: 0.0117 - accuracy: 0.9983 - val_loss: 0.7298 - val_accuracy: 0.9345\n",
            "Epoch 20/50\n",
            "250/250 [==============================] - 42s 168ms/step - loss: 1.4902e-05 - accuracy: 1.0000 - val_loss: 0.7832 - val_accuracy: 0.9355\n",
            "Epoch 21/50\n",
            "250/250 [==============================] - 42s 167ms/step - loss: 2.6259e-06 - accuracy: 1.0000 - val_loss: 0.7626 - val_accuracy: 0.9375\n",
            "Epoch 22/50\n",
            "250/250 [==============================] - 43s 170ms/step - loss: 1.6060e-06 - accuracy: 1.0000 - val_loss: 0.7698 - val_accuracy: 0.9370\n",
            "Epoch 23/50\n",
            "250/250 [==============================] - 42s 167ms/step - loss: 1.1798e-06 - accuracy: 1.0000 - val_loss: 0.7808 - val_accuracy: 0.9370\n",
            "Epoch 24/50\n",
            "250/250 [==============================] - 42s 167ms/step - loss: 1.1242e-06 - accuracy: 1.0000 - val_loss: 0.7814 - val_accuracy: 0.9365\n",
            "Epoch 25/50\n",
            "250/250 [==============================] - 43s 170ms/step - loss: 9.6840e-07 - accuracy: 1.0000 - val_loss: 0.7888 - val_accuracy: 0.9365\n",
            "Epoch 26/50\n",
            "250/250 [==============================] - 42s 169ms/step - loss: 8.4653e-07 - accuracy: 1.0000 - val_loss: 0.7951 - val_accuracy: 0.9365\n",
            "Epoch 27/50\n",
            "250/250 [==============================] - 42s 167ms/step - loss: 7.7840e-07 - accuracy: 1.0000 - val_loss: 0.7975 - val_accuracy: 0.9365\n",
            "Epoch 28/50\n",
            "250/250 [==============================] - 42s 167ms/step - loss: 7.1667e-07 - accuracy: 1.0000 - val_loss: 0.8009 - val_accuracy: 0.9365\n",
            "Epoch 29/50\n",
            "250/250 [==============================] - 42s 168ms/step - loss: 6.5425e-07 - accuracy: 1.0000 - val_loss: 0.8069 - val_accuracy: 0.9375\n",
            "Epoch 30/50\n",
            "250/250 [==============================] - 44s 175ms/step - loss: 6.0455e-07 - accuracy: 1.0000 - val_loss: 0.8121 - val_accuracy: 0.9375\n",
            "Epoch 31/50\n",
            "250/250 [==============================] - 43s 171ms/step - loss: 5.6748e-07 - accuracy: 1.0000 - val_loss: 0.8151 - val_accuracy: 0.9375\n",
            "Epoch 32/50\n",
            "250/250 [==============================] - 42s 167ms/step - loss: 5.2613e-07 - accuracy: 1.0000 - val_loss: 0.8176 - val_accuracy: 0.9380\n",
            "Epoch 33/50\n",
            "250/250 [==============================] - 42s 168ms/step - loss: 4.9336e-07 - accuracy: 1.0000 - val_loss: 0.8199 - val_accuracy: 0.9360\n",
            "Epoch 34/50\n",
            "250/250 [==============================] - 42s 168ms/step - loss: 4.5935e-07 - accuracy: 1.0000 - val_loss: 0.8228 - val_accuracy: 0.9360\n",
            "Epoch 35/50\n",
            "250/250 [==============================] - 42s 168ms/step - loss: 4.4300e-07 - accuracy: 1.0000 - val_loss: 0.8277 - val_accuracy: 0.9375\n",
            "Epoch 36/50\n",
            "250/250 [==============================] - 44s 176ms/step - loss: 4.1883e-07 - accuracy: 1.0000 - val_loss: 0.8290 - val_accuracy: 0.9365\n",
            "Epoch 37/50\n",
            "250/250 [==============================] - 42s 167ms/step - loss: 4.0182e-07 - accuracy: 1.0000 - val_loss: 0.8334 - val_accuracy: 0.9380\n",
            "Epoch 38/50\n",
            "250/250 [==============================] - 42s 167ms/step - loss: 3.8079e-07 - accuracy: 1.0000 - val_loss: 0.8378 - val_accuracy: 0.9375\n",
            "Epoch 39/50\n",
            "250/250 [==============================] - 43s 170ms/step - loss: 3.6644e-07 - accuracy: 1.0000 - val_loss: 0.8380 - val_accuracy: 0.9365\n",
            "Epoch 40/50\n",
            "250/250 [==============================] - 42s 167ms/step - loss: 3.5030e-07 - accuracy: 1.0000 - val_loss: 0.8398 - val_accuracy: 0.9365\n",
            "Epoch 41/50\n",
            "250/250 [==============================] - 42s 168ms/step - loss: 3.3877e-07 - accuracy: 1.0000 - val_loss: 0.8431 - val_accuracy: 0.9375\n",
            "Epoch 42/50\n",
            "250/250 [==============================] - 42s 167ms/step - loss: 3.2461e-07 - accuracy: 1.0000 - val_loss: 0.8462 - val_accuracy: 0.9375\n",
            "Epoch 43/50\n",
            "250/250 [==============================] - 44s 175ms/step - loss: 3.1067e-07 - accuracy: 1.0000 - val_loss: 0.8486 - val_accuracy: 0.9375\n",
            "Epoch 44/50\n",
            "250/250 [==============================] - 42s 167ms/step - loss: 3.0173e-07 - accuracy: 1.0000 - val_loss: 0.8500 - val_accuracy: 0.9380\n",
            "Epoch 45/50\n",
            "250/250 [==============================] - 42s 168ms/step - loss: 2.9048e-07 - accuracy: 1.0000 - val_loss: 0.8517 - val_accuracy: 0.9375\n",
            "Epoch 46/50\n",
            "250/250 [==============================] - 44s 175ms/step - loss: 2.7896e-07 - accuracy: 1.0000 - val_loss: 0.8523 - val_accuracy: 0.9360\n",
            "Epoch 47/50\n",
            "250/250 [==============================] - 44s 175ms/step - loss: 2.7103e-07 - accuracy: 1.0000 - val_loss: 0.8540 - val_accuracy: 0.9360\n",
            "Epoch 48/50\n",
            "250/250 [==============================] - 42s 168ms/step - loss: 2.6338e-07 - accuracy: 1.0000 - val_loss: 0.8574 - val_accuracy: 0.9380\n",
            "Epoch 49/50\n",
            "250/250 [==============================] - 42s 170ms/step - loss: 2.5288e-07 - accuracy: 1.0000 - val_loss: 0.8602 - val_accuracy: 0.9375\n",
            "Epoch 50/50\n",
            "250/250 [==============================] - 42s 167ms/step - loss: 2.4707e-07 - accuracy: 1.0000 - val_loss: 0.8609 - val_accuracy: 0.9380\n"
          ]
        }
      ],
      "source": [
        "keras_vgg16.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "r = keras_vgg16.fit(\n",
        "        train_generator,\n",
        "        epochs=50,\n",
        "        validation_data=validation_generator)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-01-15T18:05:56.858379100Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e4a148c2c534774",
        "outputId": "e50a26af-b635-4b95-b36d-34fa09d3d047"
      },
      "id": "6e4a148c2c534774"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 8s 131ms/step - loss: 0.8609 - accuracy: 0.9380\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8609157800674438, 0.9380000233650208]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "keras_vgg16.evaluate(validation_generator)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-01-15T18:03:10.512323600Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4abba1c4fb978b78",
        "outputId": "ee8cf0c1-3fef-44b5-bc1b-1f10cdc15bfb"
      },
      "id": "4abba1c4fb978b78"
    },
    {
      "cell_type": "markdown",
      "source": [
        "For InceptionV3 we are doing the same model as for a VGG16 model, replacing only a base model."
      ],
      "metadata": {
        "collapsed": false,
        "id": "a2db37ef96324b52"
      },
      "id": "a2db37ef96324b52"
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications import InceptionV3\n",
        "\n",
        "keras_inception = Sequential()\n",
        "keras_inception.add(InceptionV3(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3)))\n",
        "keras_inception.add(Flatten())\n",
        "keras_inception.add(Dense(128, activation=\"relu\"))\n",
        "keras_inception.add(Dense(64, activation=\"relu\"))\n",
        "keras_inception.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "keras_inception.layers[0].trainable = False\n",
        "\n",
        "keras_inception.summary()"
      ],
      "metadata": {
        "id": "4DhHZAYiAAb8"
      },
      "id": "4DhHZAYiAAb8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras_inception.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "r = keras_inception.fit(\n",
        "        train_generator,\n",
        "        epochs=50,\n",
        "        validation_data=validation_generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j19WmJ5ubXhb",
        "outputId": "35b56b6c-5b01-4ec3-9b11-b26102625437"
      },
      "id": "j19WmJ5ubXhb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "250/250 [==============================] - 37s 115ms/step - loss: 0.8898 - accuracy: 0.9565 - val_loss: 0.0542 - val_accuracy: 0.9885\n",
            "Epoch 2/50\n",
            "250/250 [==============================] - 25s 99ms/step - loss: 0.1252 - accuracy: 0.9822 - val_loss: 0.1679 - val_accuracy: 0.9805\n",
            "Epoch 3/50\n",
            "250/250 [==============================] - 27s 107ms/step - loss: 0.0508 - accuracy: 0.9896 - val_loss: 0.1066 - val_accuracy: 0.9865\n",
            "Epoch 4/50\n",
            "250/250 [==============================] - 25s 99ms/step - loss: 0.0244 - accuracy: 0.9945 - val_loss: 0.0890 - val_accuracy: 0.9895\n",
            "Epoch 5/50\n",
            "250/250 [==============================] - 26s 106ms/step - loss: 0.0174 - accuracy: 0.9971 - val_loss: 0.1474 - val_accuracy: 0.9895\n",
            "Epoch 6/50\n",
            "250/250 [==============================] - 25s 101ms/step - loss: 0.0166 - accuracy: 0.9977 - val_loss: 0.0976 - val_accuracy: 0.9895\n",
            "Epoch 7/50\n",
            "250/250 [==============================] - 26s 102ms/step - loss: 6.5167e-05 - accuracy: 1.0000 - val_loss: 0.1441 - val_accuracy: 0.9885\n",
            "Epoch 8/50\n",
            "250/250 [==============================] - 25s 101ms/step - loss: 0.0018 - accuracy: 0.9999 - val_loss: 0.1828 - val_accuracy: 0.9880\n",
            "Epoch 9/50\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 1.5117e-04 - accuracy: 1.0000 - val_loss: 0.1581 - val_accuracy: 0.9880\n",
            "Epoch 10/50\n",
            "250/250 [==============================] - 25s 101ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 0.1705 - val_accuracy: 0.9865\n",
            "Epoch 11/50\n",
            "250/250 [==============================] - 26s 102ms/step - loss: 0.0042 - accuracy: 0.9991 - val_loss: 0.3273 - val_accuracy: 0.9810\n",
            "Epoch 12/50\n",
            "250/250 [==============================] - 25s 100ms/step - loss: 0.0027 - accuracy: 0.9998 - val_loss: 0.1952 - val_accuracy: 0.9885\n",
            "Epoch 13/50\n",
            "250/250 [==============================] - 25s 101ms/step - loss: 1.6358e-07 - accuracy: 1.0000 - val_loss: 0.1968 - val_accuracy: 0.9890\n",
            "Epoch 14/50\n",
            "250/250 [==============================] - 26s 102ms/step - loss: 1.2233e-07 - accuracy: 1.0000 - val_loss: 0.1981 - val_accuracy: 0.9890\n",
            "Epoch 15/50\n",
            "250/250 [==============================] - 25s 102ms/step - loss: 1.0041e-07 - accuracy: 1.0000 - val_loss: 0.1993 - val_accuracy: 0.9885\n",
            "Epoch 16/50\n",
            "250/250 [==============================] - 25s 98ms/step - loss: 8.5559e-08 - accuracy: 1.0000 - val_loss: 0.2004 - val_accuracy: 0.9890\n",
            "Epoch 17/50\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 7.5021e-08 - accuracy: 1.0000 - val_loss: 0.2013 - val_accuracy: 0.9890\n",
            "Epoch 18/50\n",
            "250/250 [==============================] - 26s 105ms/step - loss: 6.6995e-08 - accuracy: 1.0000 - val_loss: 0.2020 - val_accuracy: 0.9890\n",
            "Epoch 19/50\n",
            "250/250 [==============================] - 25s 100ms/step - loss: 6.0507e-08 - accuracy: 1.0000 - val_loss: 0.2028 - val_accuracy: 0.9890\n",
            "Epoch 20/50\n",
            "250/250 [==============================] - 25s 100ms/step - loss: 5.5298e-08 - accuracy: 1.0000 - val_loss: 0.2035 - val_accuracy: 0.9890\n",
            "Epoch 21/50\n",
            "250/250 [==============================] - 26s 102ms/step - loss: 5.1033e-08 - accuracy: 1.0000 - val_loss: 0.2040 - val_accuracy: 0.9890\n",
            "Epoch 22/50\n",
            "250/250 [==============================] - 26s 102ms/step - loss: 4.7337e-08 - accuracy: 1.0000 - val_loss: 0.2045 - val_accuracy: 0.9890\n",
            "Epoch 23/50\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 4.4135e-08 - accuracy: 1.0000 - val_loss: 0.2051 - val_accuracy: 0.9890\n",
            "Epoch 24/50\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 4.1403e-08 - accuracy: 1.0000 - val_loss: 0.2056 - val_accuracy: 0.9890\n",
            "Epoch 25/50\n",
            "250/250 [==============================] - 26s 102ms/step - loss: 3.9022e-08 - accuracy: 1.0000 - val_loss: 0.2060 - val_accuracy: 0.9890\n",
            "Epoch 26/50\n",
            "250/250 [==============================] - 25s 100ms/step - loss: 3.6880e-08 - accuracy: 1.0000 - val_loss: 0.2064 - val_accuracy: 0.9890\n",
            "Epoch 27/50\n",
            "250/250 [==============================] - 26s 105ms/step - loss: 3.4958e-08 - accuracy: 1.0000 - val_loss: 0.2068 - val_accuracy: 0.9890\n",
            "Epoch 28/50\n",
            "250/250 [==============================] - 25s 100ms/step - loss: 3.3199e-08 - accuracy: 1.0000 - val_loss: 0.2071 - val_accuracy: 0.9890\n",
            "Epoch 29/50\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 3.1673e-08 - accuracy: 1.0000 - val_loss: 0.2075 - val_accuracy: 0.9890\n",
            "Epoch 30/50\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 3.0354e-08 - accuracy: 1.0000 - val_loss: 0.2078 - val_accuracy: 0.9890\n",
            "Epoch 31/50\n",
            "250/250 [==============================] - 26s 102ms/step - loss: 2.8946e-08 - accuracy: 1.0000 - val_loss: 0.2081 - val_accuracy: 0.9890\n",
            "Epoch 32/50\n",
            "250/250 [==============================] - 25s 100ms/step - loss: 2.7782e-08 - accuracy: 1.0000 - val_loss: 0.2084 - val_accuracy: 0.9890\n",
            "Epoch 33/50\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 2.6664e-08 - accuracy: 1.0000 - val_loss: 0.2087 - val_accuracy: 0.9890\n",
            "Epoch 34/50\n",
            "250/250 [==============================] - 26s 102ms/step - loss: 2.5654e-08 - accuracy: 1.0000 - val_loss: 0.2090 - val_accuracy: 0.9890\n",
            "Epoch 35/50\n",
            "250/250 [==============================] - 26s 104ms/step - loss: 2.4724e-08 - accuracy: 1.0000 - val_loss: 0.2092 - val_accuracy: 0.9890\n",
            "Epoch 36/50\n",
            "250/250 [==============================] - 26s 102ms/step - loss: 2.3858e-08 - accuracy: 1.0000 - val_loss: 0.2095 - val_accuracy: 0.9890\n",
            "Epoch 37/50\n",
            "250/250 [==============================] - 27s 107ms/step - loss: 2.3073e-08 - accuracy: 1.0000 - val_loss: 0.2097 - val_accuracy: 0.9890\n",
            "Epoch 38/50\n",
            "250/250 [==============================] - 26s 105ms/step - loss: 2.2347e-08 - accuracy: 1.0000 - val_loss: 0.2100 - val_accuracy: 0.9890\n",
            "Epoch 39/50\n",
            "250/250 [==============================] - 26s 102ms/step - loss: 2.1667e-08 - accuracy: 1.0000 - val_loss: 0.2102 - val_accuracy: 0.9890\n",
            "Epoch 40/50\n",
            "250/250 [==============================] - 26s 106ms/step - loss: 2.1026e-08 - accuracy: 1.0000 - val_loss: 0.2105 - val_accuracy: 0.9890\n",
            "Epoch 41/50\n",
            "250/250 [==============================] - 25s 101ms/step - loss: 2.0421e-08 - accuracy: 1.0000 - val_loss: 0.2107 - val_accuracy: 0.9890\n",
            "Epoch 42/50\n",
            "250/250 [==============================] - 25s 101ms/step - loss: 1.9853e-08 - accuracy: 1.0000 - val_loss: 0.2109 - val_accuracy: 0.9890\n",
            "Epoch 43/50\n",
            "250/250 [==============================] - 25s 101ms/step - loss: 1.9314e-08 - accuracy: 1.0000 - val_loss: 0.2111 - val_accuracy: 0.9890\n",
            "Epoch 44/50\n",
            "250/250 [==============================] - 27s 107ms/step - loss: 1.8806e-08 - accuracy: 1.0000 - val_loss: 0.2113 - val_accuracy: 0.9890\n",
            "Epoch 45/50\n",
            "250/250 [==============================] - 25s 101ms/step - loss: 1.8316e-08 - accuracy: 1.0000 - val_loss: 0.2115 - val_accuracy: 0.9890\n",
            "Epoch 46/50\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 1.7863e-08 - accuracy: 1.0000 - val_loss: 0.2117 - val_accuracy: 0.9890\n",
            "Epoch 47/50\n",
            "250/250 [==============================] - 25s 101ms/step - loss: 1.7428e-08 - accuracy: 1.0000 - val_loss: 0.2119 - val_accuracy: 0.9890\n",
            "Epoch 48/50\n",
            "250/250 [==============================] - 27s 109ms/step - loss: 1.7013e-08 - accuracy: 1.0000 - val_loss: 0.2121 - val_accuracy: 0.9890\n",
            "Epoch 49/50\n",
            "250/250 [==============================] - 26s 102ms/step - loss: 1.6611e-08 - accuracy: 1.0000 - val_loss: 0.2123 - val_accuracy: 0.9890\n",
            "Epoch 50/50\n",
            "250/250 [==============================] - 26s 103ms/step - loss: 1.6230e-08 - accuracy: 1.0000 - val_loss: 0.2125 - val_accuracy: 0.9890\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keras_inception.evaluate(validation_generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDNWhuDqbZUp",
        "outputId": "37eb5f2a-402a-4a10-a573-c9bee7d20ab6"
      },
      "id": "UDNWhuDqbZUp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 5s 80ms/step - loss: 0.2125 - accuracy: 0.9890\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2124653160572052, 0.9890000224113464]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For both models results are very good, after only 50 epochs, model using VGG16 has an accuracy of 0.93800. The model using InceptionV3 did even better and after same number of epochs has an accuracy of 0.98900. The second model is not only better in terms of accuracy, but also it was trained a lot faster."
      ],
      "metadata": {
        "collapsed": false,
        "id": "5937317704ee1a9d"
      },
      "id": "5937317704ee1a9d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch\n",
        "For PyTorch we use only the Inception3 as a base model. We used the same network structure as in a Keras scenario. The implementation was a bit harder as we had to write more code, but it also allows to make a lot of tuning and custom logic for a specific task. Inception3 in PyTorch takes other input sizes (299x299) than in the Keras."
      ],
      "metadata": {
        "collapsed": false,
        "id": "6e647065a088ca52"
      },
      "id": "6e647065a088ca52"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "valid_transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder('../data/dataset/training_set', transform=train_transform)\n",
        "valid_dataset = datasets.ImageFolder('../data/dataset/test_set', transform=valid_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "7nCTfkIZrgr4",
        "ExecuteTime": {
          "end_time": "2024-01-15T21:44:17.582249700Z",
          "start_time": "2024-01-15T21:44:17.267759500Z"
        }
      },
      "id": "7nCTfkIZrgr4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InceptionV3Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(InceptionV3Model, self).__init__()\n",
        "        self.base_model = models.inception_v3(pretrained=True)\n",
        "        for param in self.base_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.base_model.fc = nn.Sequential(\n",
        "            nn.Linear(2048, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training:\n",
        "            x, aux = self.base_model(x)\n",
        "            return x\n",
        "        else:\n",
        "            x = self.base_model(x)\n",
        "            return x\n",
        "\n",
        "pytorch_inceptionv3 = InceptionV3Model()"
      ],
      "metadata": {
        "id": "5KVLc3YXwek6",
        "ExecuteTime": {
          "end_time": "2024-01-15T21:45:10.856273600Z",
          "start_time": "2024-01-15T21:44:17.587261100Z"
        },
        "outputId": "798f9bfd-0ae6-4abd-f846-e0916ec099b5"
      },
      "id": "5KVLc3YXwek6",
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "D:\\Studia\\Semestr 2\\SNDL\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "D:\\Studia\\Semestr 2\\SNDL\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to C:\\Users\\piotr/.cache\\torch\\hub\\checkpoints\\inception_v3_google-0cc3c7bd.pth\n",
            "100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Loss and Optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.RMSprop(pytorch_inceptionv3.parameters(), lr=0.001)\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(10):\n",
        "    pytorch_inceptionv3.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = pytorch_inceptionv3(inputs)\n",
        "        loss = criterion(outputs, labels.float().unsqueeze(1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    pytorch_inceptionv3.eval()\n",
        "    valid_loss = 0.0\n",
        "    for inputs, labels in valid_loader:\n",
        "        outputs = pytorch_inceptionv3(inputs)\n",
        "        loss = criterion(outputs, labels.float().unsqueeze(1))\n",
        "        valid_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Validation Loss: {valid_loss / len(valid_loader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1ZZNTOqwor4",
        "outputId": "6b9e1c18-583b-4509-a0d2-ec250c4a7b2e",
        "ExecuteTime": {
          "end_time": "2024-01-16T02:09:33.988706600Z",
          "start_time": "2024-01-15T21:44:44.609563800Z"
        }
      },
      "id": "s1ZZNTOqwor4",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Validation Loss: 0.1084659102387608\n",
            "Epoch 2, Validation Loss: 0.0755010639568643\n",
            "Epoch 3, Validation Loss: 0.08579183453398329\n",
            "Epoch 4, Validation Loss: 0.10519748406543855\n",
            "Epoch 5, Validation Loss: 0.05948747944323316\n",
            "Epoch 6, Validation Loss: 0.056588030926557994\n",
            "Epoch 7, Validation Loss: 0.05583596926566864\n",
            "Epoch 8, Validation Loss: 0.07319517400381821\n",
            "Epoch 9, Validation Loss: 0.06331603878015091\n",
            "Epoch 10, Validation Loss: 0.05585289882895138\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.98\n"
          ]
        }
      ],
      "source": [
        "pytorch_inceptionv3.eval()\n",
        "accuracy = 0.0\n",
        "total = 0\n",
        "\n",
        "for inputs, labels in valid_loader:\n",
        "    outputs = pytorch_inceptionv3(inputs)\n",
        "    predicted = outputs.round()\n",
        "    total += labels.size(0)\n",
        "    accuracy += (predicted == labels.unsqueeze(1)).sum().item()\n",
        "\n",
        "print(f\"Accuracy: {accuracy / total}\")"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-16T02:13:30.725043800Z",
          "start_time": "2024-01-16T02:09:33.986594300Z"
        },
        "id": "93245e71b0fd0a2",
        "outputId": "0718eb37-fd9a-462d-d5b5-8b63fe87fac1"
      },
      "id": "93245e71b0fd0a2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to the implementation in Keras accuracy of the model using Inception3 accuracy is 98%. The training process took much more time.\n",
        "To summarize, transfer learning can help to create models much faster than with the traditional approach. The Keras and PyTorch give almost the same value of accuracy metric. The pros of Keras are that is faster to develop and faster to train, while PyTorch gives more development options and modifications in network logic.\n",
        "For this specific task, the InceptionV3 model did better in both time of execution and accuracy than the VGG16 model."
      ],
      "metadata": {
        "collapsed": false,
        "id": "413fd7646ad897d5"
      },
      "id": "413fd7646ad897d5"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}